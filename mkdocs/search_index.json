{
    "docs": [
        {
            "location": "/",
            "text": "AzureClusterlessHPC.jl - Simplified parallel computing on Azure with Julia\n\n\nOverview\n\n\nAzureClusterlessHPC.jl\n is a package for simplified batch computing in the cloud. AzureClusterlessHPC.jl borrows the syntax of \nJulia's Distributed Programming\n package to easily execute parallel Julia workloads in the cloud using batch computing services such as \nAzure Batch\n.\n\n\nInstallation and prerequisites\n\n\nTo install AzureClusterlessHPC.jl, run the following command from an interactive Julia session (press the \n]\n key and then type the command). When prompted, enter the user name and password that were provided to you:\n\n\n] add https://github.com/microsoft/AzureClusterlessHPC.jl\n\n\n\n\nAzureClusterlessHPC requires the Azure software development kits (SDKs) for batch computing, blob storage and common functionalities. See \npyrequirements.txt\n for the full list of current requirements. To install the required packages, run\n\n\n# Go to AzureClusterlessHPC directory\ncd /path/to/AzureClusterlessHPC\npip3 install -r pyrequirements.txt\n\n\n\n\nQuick start\n\n\nBefore running an example, we need to create two JSON files with our Azure credentials and the job parameters, as well as a bash startup-script for the worker nodes. Templates for these files are located in the examples directory:\n\n\n# Go to example directory\ncd /path/to/AzureClusterlessHPC/examples/batch\n\n# List directory content\nls -l\n\ncredentials.json\njulia_batch_macros.ipynb\nparameters.json\npool_startup_script.sh\n\n\n\n\nFill out the missing information in \ncredentials.json\n and in \nparameters.json\n (see the next section \"Parmeters and credentials\" for additional information). Then set the environment variables \nCREDENTIALS\n and \nPARAMETERS\n so that they point to the files. You can either set the variables in your bash terminal (e.g. in your \n~/.bashrc\n file), or directly in the Julia terminal:\n\n\n# Set path to credentials in Julia\nENV[\"CREDENTIALS\"] = joinpath(pwd(), \"credentials.json\")\n\n# Set path to batch parameters (pool id, VM types, etc.)\nENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\")\n\n\n\n\nNext, load AzureClusterlessHPC.jl and create a pool with the parameters from \nparameters.json\n:\n\n\n# Load package\nusing AzureClusterlessHPC\n\n# Create default pool with parameters from parameters.json\nstartup_script = \"pool_startup_script.sh\"\ncreate_pool_and_resource_file(startup_script)\n\n\n\n\nRemark: If a pool with the name as specified in \nparameter.json\n already exists, the \ncreate_pool_and_resource_file\n function will throw an error.In practice, use a \ntry ... catch\n block around this expression.\n\n\nNow you can execute Julia functions that are defined using the \n@batchdef\n macro via Azure batch:\n\n\n# Define function\n@batchdef function hello_world(name)\n    print(\"Hello $name\")\n    return \"Goodbye\"\nend\n\n# Execute function via Azure batch\n@batchexec hello_world(\"Bob\")\n\n\n\n\nYou can also run multi-tasks batch job using the \npmap\n function in combination with \n@batchdef\n:\n\n\n# Run a multi-task batch job\n@batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"])\n\n\n\n\nTo delete all resources run:\n\n\n# Shut down pool\ndelete_pool()\n\n# Delete container with temporary blob files\ndelete_container()\n\n\n\n\nParameters and credentials\n\n\nCredentials (one batch and storage account)\n\n\nTo use a single Azure Batch and storage account, you can set up a single combined credential file for both accounts. The required information must provided as a JSON file containing user credentials for Azure blob storage and Azure batch. Azure Batch requires authentication via the Azure Active Directory (AAD), whereas the blob storage account must be authenticated with a secret key. Refer to the \nAzure documentation\n for information on how to authenticate Azure Batch via the AAD.\n\n\nUse the following template to create a file called \ncredentials.json\n file and fill in your keys and ids. Safely store this file and never upload it to public repositories:\n\n\n{\n    \"_AD_TENANT\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\n    \"_AD_BATCH_CLIENT_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"_AD_SECRET_BATCH\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    \"_BATCH_ACCOUNT_URL\": \"https://batchaccountname.batchregion.batch.azure.com\",\n    \"_BATCH_RESOURCE\": \"https://batch.core.windows.net/\",\n\n    \"_STORAGE_ACCOUNT_NAME\": \"storageaccountname\",\n    \"_STORAGE_ACCOUNT_KEY\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n}\n\n\n\n\nWhen using AzureClusterlessHPC, set the environment variable \nENV[\"CREDENTIALS\"] = \"/path/to/credentials.json\"\n \nbefore\n you load the package via \nusing AzureClusterlessHPC\n.\n\n\nCredentials (multiple batch and storage accounts)\n\n\nAzureClusterlessHPC also allows using multiple storage and/or batch accounts. Using multiple batch accounts provides the possiblity to cirumvent service limits of a single batch account or it allows to distribute workloads among multiple regions. If you create batch accounts for multiple regions, you need to have at least one storage account in each region. To automatically create multiple batch and storage accounts, use the shell script \ncreate_azure_accounts.sh\n. Pass the list of region(s) and the number of accounts per region as command line arguments to the script. E.g., to create two batch and storeage acounts in each US West and South Central US (i.e, total of 4 batch and 4 storage accounts), run:\n\n\n# Go to AzureClusterlessHPC directory\ncd /path/to/AzureClusterlessHPC\n\n# Azure CLI log in\naz login\n\n# Create accounts\n./create_azure_accounts \"westus southcentralus\" 2\n\n\n\n\nCreating the accounts may take several minutes, depending on how many accounts are being created. The script also fetches the required credentials and stores them in the directory \nuser_data\n. No further actions from the user side are required. To use the credentials stored in \nuser_data\n with AzureClusterlessHPC, make sure that the environment variable \n\"CREDENTIALS\"\n is unset (run \nunset CREDENTIALS\n from the bash command line). If \nCREDENTIALS\n is not set, AzureClusterlessHPC will automatically look for credentials in \nuser_data\n. \n\n\nAfter loading AzureClusterlessHPC in Julia (\nusing AzureClusterlessHPC\n), you can check which accounts were found by checking \nAzureClusterlessHPC.__credentials__\n. This returns a list with one entry per available batch account. Type \nAzureClusterlessHPC.__credentials__[i]\n to print the credential information for the \ni-th\n account.\n\n\nBatch parameters\n\n\nUsers can optionally provide a \nparameters.json\n file that specifies pool and job parameters. Set the environment variable \nENV[\"PARAMETERS\"]=/path/to/parameters.json\n \nbefore\n loading the package (see section \"Quickstart\" for an example).\n\n\nThe following set of parameters and default values are used, unless specified otherwise by the user:\n\n\n{    \n    \"_POOL_ID\": \"BatchPool\",\n    \"_POOL_COUNT\": \"1\",\n    \"_NODE_COUNT_PER_POOL\": \"1\",\n    \"_POOL_VM_SIZE\": \"Standard_E2s_v3\",\n    \"_JOB_ID\": \"BatchJob\",\n    \"_STANDARD_OUT_FILE_NAME\": \"stdout.txt\",\n    \"_NODE_OS_PUBLISHER\": \"Canonical\",\n    \"_NODE_OS_OFFER\": \"UbuntuServer\",\n    \"_NODE_OS_SKU\": \"18.04\",\n    \"_BLOB_CONTAINER\": \"redwoodtemp\",\n    \"_INTER_NODE_CONNECTION\": \"0\",\n    \"_NUM_RETRYS\": \"0\",\n    \"_MPI_RUN\": \"0\",\n    \"_CONTAINER\": \"None\",\n    \"_NUM_NODES_PER_TASK\": \"1\",\n    \"_NUM_PROCS_PER_NODE\": \"1\",\n    \"_OMP_NUM_THREADS\": \"1\",\n    \"_JULIA_DEPOT_PATH\": \"/mnt/batch/tasks/startup/wd/.julia\",\n    \"_PYTHONPATH\": \"/mnt/batch/tasks/startup/wd/.local/lib/python3.6/site-packages\"\n}\n\n\n\n\nNote:\n Do not modify the \n\"_JULIA_DEPOT_PATH\"\n and \n\"_PYTHONPATH\"\n unless you use a pool with a custom image in which Julia has been already installed. In that case, set the depot path to the location of the \n.julia\n directory.\n\n\nSet up a batch pool\n\n\nStart a pool and optionally install Julia packages on the workers\n\n\nTo start a batch pool and (optionally) install a set of specified Julia packages on the workers, we first need to create a bash script of the following form, which will be executed by each node joining the pool:\n\n\n#!/bin/bash\n\n###################################################################################################\n# DO NOT MODIFY!\n\n# Switch to superuser and load module\nsudo bash\npwd\n\n# Install Julia\nwget \"https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.2-linux-x86_64.tar.gz\"\ntar -xvzf julia-1.5.2-linux-x86_64.tar.gz\nrm -rf julia-1.5.2-linux-x86_64.tar.gz\nln -s /mnt/batch/tasks/startup/wd/julia-1.5.2/bin/julia /usr/local/bin/julia\n\n# Install AzureClusterlessHPC\ngit clone https://github.com/microsoft/AzureClusterlessHPC.jl\njulia -e 'using Pkg; Pkg.add(url=joinpath(pwd(), \"AzureClusterlessHPC\"))'\n\n###################################################################################################\n# ADD USER PACKAGES HERE\n# ...\n\n###################################################################################################\n# DO NOT MODIFY!\n\n# Make julia dir available for all users\nchmod -R 777 /mnt/batch/tasks/startup/wd/.julia\n\n\n\n\n\nIf you need to install Julia packages for your application, specify the packages in the section \n# ADD USER PACKAGES HERE\n. E.g. to install the Julia package \nIterativeSolvers.jl\n, add the line:\n\n\njulia -e 'using Pkg; Pkg.add(\"IterativeSolvers\")'\n\n\n\n\nTo install packages that are not officially registered with Julia, use this line to add packages:\n\n\njulia -e 'using Pkg; Pkg.develop(PackageSpec(url=\"https://github.com/slimgroup/JOLI.jl\"))'\n\n\n\n\nSave this batch script, e.g. as \npool_startup_script.sh\n. You can now create a pool in which the startup script will be executed on each node that joins the pool:\n\n\n# Path to bash file\nstartup_script = \"/path/to/pool_startup_script.sh\"\n\n# Create pool\ncreate_pool_and_resource_file(startup_script; enable_auto_scale=false, auto_scale_formula=nothing,\n    auto_scale_evaluation_interval_minutes=nothing, image_resource_id=nothing)\n\n\n\n\nRequired input arguments:\n\n\n\n\nstartup_script\n: String that defines the path and name of the bash startup script.\n\n\n\n\nOptional keyword arguments\n:\n\n\n\n\n\n\nenable_auto_scale=false\n: Enable auto scaling. If \ntrue\n, the keyword arguments \nauto_scale_formula\n and \nauto_scale_evaluation_interval_minutes\n must be provided as well. If the parameter \n_POOL_NODE_COUNT\n has been set, it will be ignored.\n\n\n\n\n\n\nauto_scale_formula=nothing\n: String that defines the auto-scaling behavior. See \nhere\n for Azure Batch auto-scaling templates.\n\n\n\n\n\n\nauto_scale_evaluation_interval_minutes=nothing\n: Time interval between evaluations of the auto-scaling function. The minimum possible interval is 5 minutes.\n\n\n\n\n\n\nimage_resource_id=nothing\n: Provide an optional image resource ID to use a custom machine image for nodes joining the batch pool.\n\n\n\n\n\n\nStart a pool using an existing VM image\n\n\nTo launch a pool with a custom VM image, you need to create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see \nhere\n for details on how to create a shared image).\n\n\nOnce you have the shared image ID, pass it as a keyword argument \nimage_resource_id\n to the \ncreate_pool\n function. If you do not pass the image ID to the function, workers are created with the default Ubuntu image, which does not have Julia installed.\n\n\n# Image resource ID\nimage_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n# Create pool with custom VM image\ncreate_pool(image_resource_id=image_id, enable_auto_scale=false, auto_scale_formula=nothing,\n    auto_scale_evaluation_interval_minutes=nothing)\n\n\n\n\nOptional keyword arguments:\n\n\n\n\nimage_resource_id=nothing\n: Image resource ID to use a custom machine image for nodes joining the batch pool.\n\n\n\n\nFor a description of all other keyword arguments, see the above section.\n\n\nImportant\n: In your parameter file, set the variable \n\"_JULIA_DEPOT_PATH\"\n to the path where Julia is installed on the image.\n\n\nStart a pool using a Docker image\n\n\nAs a third alternative, you can create an application package using Docker. You first create or specify a Docker image, which will then be pre-installed on each VM joining the batch pool. See the example directory \n/path/to/redwood/examples/container\n for an example Dockerfile. Follow the subsequent instructions to create a Docker image from a Dockerfile and upload it to your (personal) container repository:\n\n\n# Move to directory with Dockerfile\ncd /path/to/redwood/examples/container\n\n# Build image\ndocker build -t redoowd:v1.0 .\n\n# Login\ndocker login\n\n# Tag and push\ndocker tag redwood:v1.0 username/redwood:v1.0\ndocker push username/redwood:v1.0\n\n\n\n\nOnce you have a Docker image in a public repository, you can specify a Docker image in your \nparameters.json\n file:\n\n\n    \"_CONTAINER\": \"username/redwood:v1.0\"\n\n\n\n\nIf the \n_CONTAINER\n parameter is set, AzureClusterlessHPC will install the specified container image on the VMs in the batch pool.\n\n\nPools with auto-scaling\n\n\nTo create a pool with auto-scaling, use one of the above commands and set the following keyword arguments:\n\n\n\n\n\n\nSet the keyword argument \nenable_auto_scale=true\n\n\n\n\n\n\nDefine an auto-scaling formula. E.g. the following formula creates a pool with 1 node and resizes the pool to up to 10 VMs based on the number of pending tasks:\n\n\n\n\n\n\nauto_scale_formula = \"\"\"startingNumberOfVMs = 1;\n    maxNumberofVMs = 10;\n    pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second);\n    pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second));\n    \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples);\n    \\$NodeDeallocationOption = taskcompletion;\"\"\"\n\n\n\n\nFor other auto-scaling formulas, refer to the \nAzure Batch documentation\n.\n\n\n\n\nSet the auto-scaling interval: \nauto_scale_evaluation_interval_minutes=5\n. The minimum allowed values is 5 minutes.\n\n\n\n\nThe full example would look like this:\n\n\n# Pool startup script\nstartup_script = \"/path/to/pool_startup_script.sh\"\n\n# Autoscale formula\nauto_scale_formula = \"\"\"startingNumberOfVMs = 1;\n    maxNumberofVMs = 10;\n    pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second);\n    pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second));\n    \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples);\n    \\$NodeDeallocationOption = taskcompletion;\"\"\"\n\ncreate_pool_and_resource_file(startup_script; enable_auto_scale=true, auto_scale_formula=auto_scale_formula,            \n    auto_scale_evaluation_interval_minutes=5)\n\n\n\n\nResize the pool\n\n\nCurrently not supported.\n\n\nRemote function calls via batch\n\n\n\\@batchdef\n\n\nExecute an expression under Main and on the batch workers of a (future) batch job that is executed from the same Julia session (equivalent to \n@everywhere\n for parallel Julia sessions).\n\n\n@batchdef expr\n\n\n\n\n@batchdef\n can be used to define variables, functions or with \ninclude\n and \nusing\n statements:\n\n\n# Import packages\n@batchdef using LinearAlgebra, Random\n\n# Includes\n@batchdef include(\"testfile.jl\")\n\n# Define variables\n@batchdef A = ones(2, 2)\n\n# Define functions\n@batchdef hello_world(name) = print(\"Hello $name\")\n\n\n\n\nYou can define multiple expression with \n@batchdef\n using a \nbegin ... end\n block:\n\n\n@batchdef begin\n    A = ones(1, 1)\n    B = zeros(1, 1)\nend\n\n\n\n\nExpressions that are tagged via \n@batchdef\n are collected by AzureClusterlessHPC and are used in subsequent batch job executions. To print the current collection of expressions, type \nbatch_show()\n. To reset the batch environment and remove all prior expressions from the call stack, use \nbatch_clear()\n (or restart the Julia session).\n\n\n\\@batchexec\n\n\nExecute an expression as a batch job (equivalent to \n@spawn\n for parallel Julia sessions).\n\n\n@batchexec expr\n\n\n\n\nThe primary purpose of \n@batchexec\n is to execute functions that have been priorly defined with \n@batchdef\n. E.g.\n\n\n# Define function\n@batchdef function hello_world(name)\n    print(\"Hello $name\")\n    return \"Goodbye\"\nend\n\n# Call function via batch\nbctrl = @batchexec hello_world(\"Bob\")\n\n\n\n\nArguments for functions executed via \n@batchexec\n are always \npassed by copy\n. This is important to keep in mind when passing large arguments to a function that is executed as a multi-task batch job, in which case arguments are copied to each task separately. To pass large arguments to a multi-task batch job, use the \n@bcast\n macro (see next section).\n\n\nTo execute a multi-task batch job, use the \npmap\n function:\n\n\n# Multi-task batch job\nbctrl = @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"])\n\n\n\n\nThe \n@batchexec\n macro returns a batch controller (\nbctrl\n) that can be used for the following actions:\n\n\n\n\n\n\nWait for all tasks of the batch job to finish: \nwait_for_tasks_to_complete(bctrl)\n\n\n\n\n\n\nTerminate the batch job: \nterminate_job(bctrl)\n\n\n\n\n\n\nDelete the batch job: \ndelete_job(bctrl)\n\n\n\n\n\n\nDelete the pool: \ndelete_pool(bctrl)\n\n\n\n\n\n\nDelete the blob container in which all temporary files are stored: \ndelete_container(bctrl)\n\n\n\n\n\n\nDestroy all Azure resources associated with the batch controller (job, pool, container): \ndestroy!(bctrl)\n\n\n\n\n\n\nFetch the output of all tasks: \noutput = fetch(bctrl)\n. This operation is blocking and waits for all tasks to finish. The output is collected asynchonously in order of completion.\n\n\n\n\n\n\nFetch the output of task \ni\n: \noutput = fetch(bctrl, i)\n (blocking for that task).\n\n\n\n\n\n\nInplace fetch (all tasks). Returns output and overwrites the blob future in \nbctrl.output\n: \noutput = fetch!(bctrl)\n (blocking operation)\n\n\n\n\n\n\nInplace fetch (task \ni\n): \noutput = fetch!(bctrl, i)\n (blocking for task \ni\n)\n\n\n\n\n\n\nFetch output of all tasks and apply a reduction operation to the output (along tasks): \noutput_reduce = fetchreduce(bctrl; op=+)\n (blocking)\n\n\n\n\n\n\nInplace fetch and reduce (overwrite \noutput_reduce\n): \nfetchreduce!(bctrl, output_reduce; op=+)\n (blocking)\n\n\n\n\n\n\nLimitations:\n\n\n\n\n\n\nFunction return arguments must be explicitley returned via the \nreturn\n statement. I.e., implicit returns in which the final function expression is automatically returned are not supported.\n\n\n\n\n\n\nFunctions executed via \n@batchexec\n can only have a single \nreturn\n argument. I.e. control structures such as \nif ... else ... end\n with multiple \nreturn\n statements are not supported and will throw an exception when fetching the output.\n\n\n\n\n\n\nFunction arguments are passed by copy, never by reference.\n\n\n\n\n\n\nMPI support\n\n\nYou can execute tasks via Julia MPI on either single VMs or on multiple VMs. To enable MPI on a single VM (shared memory parallelism), set the following variables in your \nparameters.json\n file:\n\n\n    \"_INTER_NODE_CONNECTION\": \"0\",\n    \"_MPI_RUN\": \"1\",\n    \"_NUM_NODES_PER_TASK\": \"1\",\n    \"_NUM_PROCS_PER_NODE\": \"2\",\n    \"_OMP_NUM_THREADS\": \"1\"\n\n\n\n\nNote, that \n\"_NUM_NODES_PER_TASK\"\n must be set to \n1\n if \n\"_INTER_NODE_CONNECTION\"\n is set to \n\"0\"\n. \n\"_NUM_PROCS_PER_NODE\"\n specifies the number of MPI ranks per node and \n\"_OMP_NUM_THREADS\"\n specifies the number of OpenMP threads per rank (if applicable).\n\n\nTo enable MPI tasks on multiple instances (distributed memory parallelism), set:\n\n\n    \"_INTER_NODE_CONNECTION\": \"1\",\n    \"_MPI_RUN\": \"1\",\n    \"_NUM_NODES_PER_TASK\": \"2\",\n    \"_NUM_PROCS_PER_NODE\": \"4\",\n    \"_OMP_NUM_THREADS\": \"1\"\n\n\n\n\nThe total number of MPI ranks for each task is given by \n\"_NUM_NODES_PER_TASK\"\n times \n\"_NUM_PROCS_PER_NODE\"\n. E.g. in this example, each MPI task is executed on 2 nodes with 4 processes per node, i.e. 8 MPI ranks in total.\n\n\nIn your application, you need to load the Julia MPI package via \n@batchdef\n. For a full MPI example, see \nAzureClusterlessHPC/examples/mpi/julia_batch_mpi.ipynb\n.\n\n\nBroadcasting\n\n\nBroadcast an expression to all batch workers of (future) batch jobs and return a batch future. The batch future can be passed as a function argument instead of the variable.\n\n\nbatch_future = @bcast expr\n\n\n\n\nThe use of \n@bcast\n is recommended to pass large arguments to functions (e.g. arrays). This avoids copying input arguments to each individual task separately. Instead, expressions tagged via \n@batchdef\n are uploaded to blob storage once and their blob reference is passed to one or multiple tasks.\n\n\nTo access a broadcasted variable inside an executed function, use the \nfetch\n or \nfetch!\n (in-place) function:\n\n\n# Create and broadcast array\nA = randn(2, 2)\n_A = @bcast A\n\n# Define function\n@batchdef function print_array(_A)\n    A = fetch(_A)   # load A into memory\n    print(A)\nend\n\n# Remotely execute function\n@batchexec print_array(_A)  # pass batch future\n\n\n\n\nCalling \nA = fetch(_A)\n on the local machine (rather than on a batch worker) downloads the broadcasted variable from blob storage and returns it.\n\n\nCollect output\n\n\nFetch output\n\n\nExecuting a function as a batch job via \n@batchexec\n returns a batch controller of type \nBatchController\n:\n\n\n# Test function\n@batchdef function hello_world(n)\n    A = zeros(n, n)\n    B = ones(n, n)\n    return A, B\nend\n\n# Execute function as a multi-task batch job\nn = 2\nbatch_controller = @batchexec pmap(() -> hello_world(n), 1:2)  # 2 tasks\n\n\n\n\nThe batch controller has a field called \nbatch_controller.output\n, which is a cell array of blob futures. The blob futures contain a (randomly generated) blob name of the future result stored in blob storage. E.g.:\n\n\njulia> batch_controller.output\n\n2-element Array{Any,1}:\n BlobFuture(\"redwoodtemp\", BlobRef((\"o9UspZStMmqn\", \"TwIMfLrYiac2\")))\n BlobFuture(\"redwoodtemp\", BlobRef((\"PxgtEgZonWPJ\", \"kZz1Wuknnag0\")))\n\n\n\n\nThe cell array contains one entry per task, i.e. \nlength(batch_controller.output)\n is equal to the number of tasks of the executed batch job (in this case 2). As our function returns two arguments, each \nBlobRef\n contains two (future) blob names.\n\n\nTo fetch the output of an executed function, AzureClusterlessHPC provides the \nfetch\n and \nfetch!\n functions. These functions can be either called on the batch controller \noutput = fetch(batch_controller)\n or they can be directly called on the blob futures:\n\n\n# fetch called on batch controller\noutput_job = fetch(batch_controller)\n\n# fetch called on blob future\noutput_task_1 = fetch(batch_controller.output[1])\n\n\n\n\nHowever, we recommend to always call \nfetch\n on the batch controller and not on the batch futures in \n.output\n. Calling \nfetch(batch_controller)\n is a blocking operation and waits for all batch tasks to terminate. Calling \nfetch(batch_controller.output[1])\n is non-blocking and throws an exception if the task or job has not yet finished and the output is not yet available in blob storage.\n\n\nAzureClusterlessHPC also supplies in-place fetch functions, which not only return the output, but they also overwrite the \nBlobRef\n of the \nBlobFuture\n in \nbatch_controller.output\n:\n\n\n# Inplace fetch\noutput = fetch!(batch_controller)\n\n2-element Array{Any,1}:\n ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])\n ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])\n\nbatch_controller.output\n\n2-element Array{Any,1}:\n BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]))\n BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]))\n\n\n\n\nInplace \nfetch!\n by default deletes the referenced blob objects. If \nfetch!\n is called on the batch controller again, it will then throw an error. To avoid deleting the blob, call \nfetch!(batch_controller; destroy_blob=false)\n. \n\n\nFetch output and apply reduction operation\n\n\nAzureClusterlessHPC supplies the \nfetchreduce\n and \nfetchreduce!\n functions to collect the output from multiple tasks and apply a specified reduction operation to the output.\nE.g. using the prior example:\n\n\n# Test function\n@batchdef function hello_world(n)\n    A = ones(n, n)\n    B = 2 .* ones(n, n)\n    return A, B\nend\n\n# Execute function as a multi-task batch job\nn = 2\nbatch_controller = @batchexec pmap(() -> hello_world(n), 1:2)  # 2 tasks\n\n\n\n\nWe can fetch and sum the output via:\n\n\noutput_sum = fetchreduce(batch_controller; op=+, remote=false)\n\n# Returns\n([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0])\n\n\n\n\nThe \nremote\n keyword argument specifies where the summation is execute. By default, the output is collected and summed on the master. For \nremote=true\n, AzureClusterlessHPC will schedule the summation tasks on idle instances in the batch pool and only the final (reduced) argument is copied back to the master.\n\n\nWe can also initialize the output ourselves and then call the in-place \nfetchreduce!\n function:\n\n\n# Initialize output\noutput = (zeros(2, 2), zeros(2, 2))\n\n# Fetch output and sum\nfetchreduce!(batch_controller, output; op=+)\n\n@show output\noutput = ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0])\n\n\n\n\nClean up resources\n\n\nAfter executing a batch job via \n@batchexec\n, you can use the returned batch controller to clean up resources:\n\n\n# Batch job\nbatch_controller = @batchexec print(\"Hello world\")\n\n# Terminate job\nterminate_job(batch_controller)\n\n# Delete job\ndelete_job(batch_controller)\n\n# Delete pool\ndelete_pool(batch_controller)\n\n# Delete blob container with all temporary files\ndelte_container(batch_controller)\n\n# Or alternatively, delete pool + job + container together\ndestroy!(batch_controller)\n\n\n\n\nIf you did not return a batch controller, you can call the following functions without any input arguments, in which case they will delete the pool and container as specified in your \nparameter.json\n file (or the default ones). The \ndelete_all_jobs\n function will delete all exisiting jobs that start with the job id specified in the parameter file.\n\n\n# Delete container\ndelete_container()\n\n# Delete pool\ndelete_pool()\n\n# Delete all jobs\ndelete_all_jobs()\n\n\n\n\nFAQ\n\n\n\n\nHow does AzureClusterlessHPC work?\n\n\n\n\nWhenever you tag an expression with \n@batchdef\n, AzureClusterlessHPC collects the abstract syntax tree (AST) of the expressions and appends it to a global collection. You can print the currently collected AST via \nbatch_show()\n and you can reset the collected expressions via \nbatch_clear()\n. When you use \n@batchexec\n, AzureClusterlessHPC creates a closure around the executed expression and uploads it, along with the collected AST as a batch resource file. AzureClusterlessHPC also anayzes the executed funtion and replaces return statements with serializations, so that return arguments are written to the local disk of the batch worker and subsequently uploaded to blob storage, from where they can be collected via the \nfetch\n/\nfetch!\n functions.\n\n\n\n\nWhat costs does AzureClusterlessHPC incur?\n\n\n\n\nAzureClusterlessHPC calls Azure Batch and Azure Blob Storage APIs. Costs incur for operations that write data to blob storage, download or store it (e.g. \n@bcast\n, \n@batchexec\n, \nfetch\n, \nfetch!\n). For batch jobs, costs incur for the requested VMs in the batch pool (regardless of whether jobs are currently running or not). \n\n\n\n\nHow do I clean up and shut down all services that invoke costs?\n\n\n\n\nCosts are invoked by a batch pool made up of one or multiple VMs and by files stored in blob storage. To shut down the pool run \ndelete_pool\n and to delete the blob container that contains any temporary files run \ndelete_container()\n. These actions will delete the pool and blob container specified in your parameter JSON file (or the default ones created by AzureClusterlessHPC).\n\n\n\n\nHow can I specify Julia packages to be installed on the batch worker nodes?\n\n\n\n\nTo specify Julia packages that are installed on the worker nodes, create a pool startup script and use the \ncreate_pool_and_resource_file\n function to launch the pool. Refer to the section \"Create a batch pool\" for details.\n\n\n\n\nHow can I start a pool with a custom VM image?\n\n\n\n\nTo start a pool with a custom VM image, you need to first create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see \nhere\n for details on how to create a shared image). When starting your batch pool, pass this ID to the pool startup function: \ncreate_pool(image_resource_id=\"shared_image_id\")\n.\n\n\n\n\nWhat kind of input and return arguments are supported in functions executed via \n@batchexec\n?\n\n\n\n\nAzureClusterlessHPC.jl supports any kind of input and return arguments, including custom data structures. Input and return arguments do not need to be JSON serializable. However, we recommend using the same Julia version on the batch workers as on your local machine or master VM. This avoids possible inconsistencies when serializing/deserializing arguments and expressions.\n\n\n\n\nAre MPI and multi-node batch tasks supported?\n\n\n\n\nYes, you can execute AzureClusterlessHPC tasks via Julia MPI on either single VMs or on multiple VMs. See the above section \nMPI support\n for details on how to runs batch tasks with MPI support.\n\n\nTroubleshooting\n\n\nContact the developer at \npwitte@microsoft.com\n.",
            "title": "Home"
        },
        {
            "location": "/#azureclusterlesshpcjl-simplified-parallel-computing-on-azure-with-julia",
            "text": "",
            "title": "AzureClusterlessHPC.jl - Simplified parallel computing on Azure with Julia"
        },
        {
            "location": "/#overview",
            "text": "AzureClusterlessHPC.jl  is a package for simplified batch computing in the cloud. AzureClusterlessHPC.jl borrows the syntax of  Julia's Distributed Programming  package to easily execute parallel Julia workloads in the cloud using batch computing services such as  Azure Batch .",
            "title": "Overview"
        },
        {
            "location": "/#installation-and-prerequisites",
            "text": "To install AzureClusterlessHPC.jl, run the following command from an interactive Julia session (press the  ]  key and then type the command). When prompted, enter the user name and password that were provided to you:  ] add https://github.com/microsoft/AzureClusterlessHPC.jl  AzureClusterlessHPC requires the Azure software development kits (SDKs) for batch computing, blob storage and common functionalities. See  pyrequirements.txt  for the full list of current requirements. To install the required packages, run  # Go to AzureClusterlessHPC directory\ncd /path/to/AzureClusterlessHPC\npip3 install -r pyrequirements.txt",
            "title": "Installation and prerequisites"
        },
        {
            "location": "/#quick-start",
            "text": "Before running an example, we need to create two JSON files with our Azure credentials and the job parameters, as well as a bash startup-script for the worker nodes. Templates for these files are located in the examples directory:  # Go to example directory\ncd /path/to/AzureClusterlessHPC/examples/batch\n\n# List directory content\nls -l\n\ncredentials.json\njulia_batch_macros.ipynb\nparameters.json\npool_startup_script.sh  Fill out the missing information in  credentials.json  and in  parameters.json  (see the next section \"Parmeters and credentials\" for additional information). Then set the environment variables  CREDENTIALS  and  PARAMETERS  so that they point to the files. You can either set the variables in your bash terminal (e.g. in your  ~/.bashrc  file), or directly in the Julia terminal:  # Set path to credentials in Julia\nENV[\"CREDENTIALS\"] = joinpath(pwd(), \"credentials.json\")\n\n# Set path to batch parameters (pool id, VM types, etc.)\nENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\")  Next, load AzureClusterlessHPC.jl and create a pool with the parameters from  parameters.json :  # Load package\nusing AzureClusterlessHPC\n\n# Create default pool with parameters from parameters.json\nstartup_script = \"pool_startup_script.sh\"\ncreate_pool_and_resource_file(startup_script)  Remark: If a pool with the name as specified in  parameter.json  already exists, the  create_pool_and_resource_file  function will throw an error.In practice, use a  try ... catch  block around this expression.  Now you can execute Julia functions that are defined using the  @batchdef  macro via Azure batch:  # Define function\n@batchdef function hello_world(name)\n    print(\"Hello $name\")\n    return \"Goodbye\"\nend\n\n# Execute function via Azure batch\n@batchexec hello_world(\"Bob\")  You can also run multi-tasks batch job using the  pmap  function in combination with  @batchdef :  # Run a multi-task batch job\n@batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"])  To delete all resources run:  # Shut down pool\ndelete_pool()\n\n# Delete container with temporary blob files\ndelete_container()",
            "title": "Quick start"
        },
        {
            "location": "/#parameters-and-credentials",
            "text": "",
            "title": "Parameters and credentials"
        },
        {
            "location": "/#credentials-one-batch-and-storage-account",
            "text": "To use a single Azure Batch and storage account, you can set up a single combined credential file for both accounts. The required information must provided as a JSON file containing user credentials for Azure blob storage and Azure batch. Azure Batch requires authentication via the Azure Active Directory (AAD), whereas the blob storage account must be authenticated with a secret key. Refer to the  Azure documentation  for information on how to authenticate Azure Batch via the AAD.  Use the following template to create a file called  credentials.json  file and fill in your keys and ids. Safely store this file and never upload it to public repositories:  {\n    \"_AD_TENANT\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\n    \"_AD_BATCH_CLIENT_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"_AD_SECRET_BATCH\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    \"_BATCH_ACCOUNT_URL\": \"https://batchaccountname.batchregion.batch.azure.com\",\n    \"_BATCH_RESOURCE\": \"https://batch.core.windows.net/\",\n\n    \"_STORAGE_ACCOUNT_NAME\": \"storageaccountname\",\n    \"_STORAGE_ACCOUNT_KEY\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n}  When using AzureClusterlessHPC, set the environment variable  ENV[\"CREDENTIALS\"] = \"/path/to/credentials.json\"   before  you load the package via  using AzureClusterlessHPC .",
            "title": "Credentials (one batch and storage account)"
        },
        {
            "location": "/#credentials-multiple-batch-and-storage-accounts",
            "text": "AzureClusterlessHPC also allows using multiple storage and/or batch accounts. Using multiple batch accounts provides the possiblity to cirumvent service limits of a single batch account or it allows to distribute workloads among multiple regions. If you create batch accounts for multiple regions, you need to have at least one storage account in each region. To automatically create multiple batch and storage accounts, use the shell script  create_azure_accounts.sh . Pass the list of region(s) and the number of accounts per region as command line arguments to the script. E.g., to create two batch and storeage acounts in each US West and South Central US (i.e, total of 4 batch and 4 storage accounts), run:  # Go to AzureClusterlessHPC directory\ncd /path/to/AzureClusterlessHPC\n\n# Azure CLI log in\naz login\n\n# Create accounts\n./create_azure_accounts \"westus southcentralus\" 2  Creating the accounts may take several minutes, depending on how many accounts are being created. The script also fetches the required credentials and stores them in the directory  user_data . No further actions from the user side are required. To use the credentials stored in  user_data  with AzureClusterlessHPC, make sure that the environment variable  \"CREDENTIALS\"  is unset (run  unset CREDENTIALS  from the bash command line). If  CREDENTIALS  is not set, AzureClusterlessHPC will automatically look for credentials in  user_data .   After loading AzureClusterlessHPC in Julia ( using AzureClusterlessHPC ), you can check which accounts were found by checking  AzureClusterlessHPC.__credentials__ . This returns a list with one entry per available batch account. Type  AzureClusterlessHPC.__credentials__[i]  to print the credential information for the  i-th  account.",
            "title": "Credentials (multiple batch and storage accounts)"
        },
        {
            "location": "/#batch-parameters",
            "text": "Users can optionally provide a  parameters.json  file that specifies pool and job parameters. Set the environment variable  ENV[\"PARAMETERS\"]=/path/to/parameters.json   before  loading the package (see section \"Quickstart\" for an example).  The following set of parameters and default values are used, unless specified otherwise by the user:  {    \n    \"_POOL_ID\": \"BatchPool\",\n    \"_POOL_COUNT\": \"1\",\n    \"_NODE_COUNT_PER_POOL\": \"1\",\n    \"_POOL_VM_SIZE\": \"Standard_E2s_v3\",\n    \"_JOB_ID\": \"BatchJob\",\n    \"_STANDARD_OUT_FILE_NAME\": \"stdout.txt\",\n    \"_NODE_OS_PUBLISHER\": \"Canonical\",\n    \"_NODE_OS_OFFER\": \"UbuntuServer\",\n    \"_NODE_OS_SKU\": \"18.04\",\n    \"_BLOB_CONTAINER\": \"redwoodtemp\",\n    \"_INTER_NODE_CONNECTION\": \"0\",\n    \"_NUM_RETRYS\": \"0\",\n    \"_MPI_RUN\": \"0\",\n    \"_CONTAINER\": \"None\",\n    \"_NUM_NODES_PER_TASK\": \"1\",\n    \"_NUM_PROCS_PER_NODE\": \"1\",\n    \"_OMP_NUM_THREADS\": \"1\",\n    \"_JULIA_DEPOT_PATH\": \"/mnt/batch/tasks/startup/wd/.julia\",\n    \"_PYTHONPATH\": \"/mnt/batch/tasks/startup/wd/.local/lib/python3.6/site-packages\"\n}  Note:  Do not modify the  \"_JULIA_DEPOT_PATH\"  and  \"_PYTHONPATH\"  unless you use a pool with a custom image in which Julia has been already installed. In that case, set the depot path to the location of the  .julia  directory.",
            "title": "Batch parameters"
        },
        {
            "location": "/#set-up-a-batch-pool",
            "text": "",
            "title": "Set up a batch pool"
        },
        {
            "location": "/#start-a-pool-and-optionally-install-julia-packages-on-the-workers",
            "text": "To start a batch pool and (optionally) install a set of specified Julia packages on the workers, we first need to create a bash script of the following form, which will be executed by each node joining the pool:  #!/bin/bash\n\n###################################################################################################\n# DO NOT MODIFY!\n\n# Switch to superuser and load module\nsudo bash\npwd\n\n# Install Julia\nwget \"https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.2-linux-x86_64.tar.gz\"\ntar -xvzf julia-1.5.2-linux-x86_64.tar.gz\nrm -rf julia-1.5.2-linux-x86_64.tar.gz\nln -s /mnt/batch/tasks/startup/wd/julia-1.5.2/bin/julia /usr/local/bin/julia\n\n# Install AzureClusterlessHPC\ngit clone https://github.com/microsoft/AzureClusterlessHPC.jl\njulia -e 'using Pkg; Pkg.add(url=joinpath(pwd(), \"AzureClusterlessHPC\"))'\n\n###################################################################################################\n# ADD USER PACKAGES HERE\n# ...\n\n###################################################################################################\n# DO NOT MODIFY!\n\n# Make julia dir available for all users\nchmod -R 777 /mnt/batch/tasks/startup/wd/.julia  If you need to install Julia packages for your application, specify the packages in the section  # ADD USER PACKAGES HERE . E.g. to install the Julia package  IterativeSolvers.jl , add the line:  julia -e 'using Pkg; Pkg.add(\"IterativeSolvers\")'  To install packages that are not officially registered with Julia, use this line to add packages:  julia -e 'using Pkg; Pkg.develop(PackageSpec(url=\"https://github.com/slimgroup/JOLI.jl\"))'  Save this batch script, e.g. as  pool_startup_script.sh . You can now create a pool in which the startup script will be executed on each node that joins the pool:  # Path to bash file\nstartup_script = \"/path/to/pool_startup_script.sh\"\n\n# Create pool\ncreate_pool_and_resource_file(startup_script; enable_auto_scale=false, auto_scale_formula=nothing,\n    auto_scale_evaluation_interval_minutes=nothing, image_resource_id=nothing)  Required input arguments:   startup_script : String that defines the path and name of the bash startup script.   Optional keyword arguments :    enable_auto_scale=false : Enable auto scaling. If  true , the keyword arguments  auto_scale_formula  and  auto_scale_evaluation_interval_minutes  must be provided as well. If the parameter  _POOL_NODE_COUNT  has been set, it will be ignored.    auto_scale_formula=nothing : String that defines the auto-scaling behavior. See  here  for Azure Batch auto-scaling templates.    auto_scale_evaluation_interval_minutes=nothing : Time interval between evaluations of the auto-scaling function. The minimum possible interval is 5 minutes.    image_resource_id=nothing : Provide an optional image resource ID to use a custom machine image for nodes joining the batch pool.",
            "title": "Start a pool and optionally install Julia packages on the workers"
        },
        {
            "location": "/#start-a-pool-using-an-existing-vm-image",
            "text": "To launch a pool with a custom VM image, you need to create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see  here  for details on how to create a shared image).  Once you have the shared image ID, pass it as a keyword argument  image_resource_id  to the  create_pool  function. If you do not pass the image ID to the function, workers are created with the default Ubuntu image, which does not have Julia installed.  # Image resource ID\nimage_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n# Create pool with custom VM image\ncreate_pool(image_resource_id=image_id, enable_auto_scale=false, auto_scale_formula=nothing,\n    auto_scale_evaluation_interval_minutes=nothing)  Optional keyword arguments:   image_resource_id=nothing : Image resource ID to use a custom machine image for nodes joining the batch pool.   For a description of all other keyword arguments, see the above section.  Important : In your parameter file, set the variable  \"_JULIA_DEPOT_PATH\"  to the path where Julia is installed on the image.",
            "title": "Start a pool using an existing VM image"
        },
        {
            "location": "/#start-a-pool-using-a-docker-image",
            "text": "As a third alternative, you can create an application package using Docker. You first create or specify a Docker image, which will then be pre-installed on each VM joining the batch pool. See the example directory  /path/to/redwood/examples/container  for an example Dockerfile. Follow the subsequent instructions to create a Docker image from a Dockerfile and upload it to your (personal) container repository:  # Move to directory with Dockerfile\ncd /path/to/redwood/examples/container\n\n# Build image\ndocker build -t redoowd:v1.0 .\n\n# Login\ndocker login\n\n# Tag and push\ndocker tag redwood:v1.0 username/redwood:v1.0\ndocker push username/redwood:v1.0  Once you have a Docker image in a public repository, you can specify a Docker image in your  parameters.json  file:      \"_CONTAINER\": \"username/redwood:v1.0\"  If the  _CONTAINER  parameter is set, AzureClusterlessHPC will install the specified container image on the VMs in the batch pool.",
            "title": "Start a pool using a Docker image"
        },
        {
            "location": "/#pools-with-auto-scaling",
            "text": "To create a pool with auto-scaling, use one of the above commands and set the following keyword arguments:    Set the keyword argument  enable_auto_scale=true    Define an auto-scaling formula. E.g. the following formula creates a pool with 1 node and resizes the pool to up to 10 VMs based on the number of pending tasks:    auto_scale_formula = \"\"\"startingNumberOfVMs = 1;\n    maxNumberofVMs = 10;\n    pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second);\n    pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second));\n    \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples);\n    \\$NodeDeallocationOption = taskcompletion;\"\"\"  For other auto-scaling formulas, refer to the  Azure Batch documentation .   Set the auto-scaling interval:  auto_scale_evaluation_interval_minutes=5 . The minimum allowed values is 5 minutes.   The full example would look like this:  # Pool startup script\nstartup_script = \"/path/to/pool_startup_script.sh\"\n\n# Autoscale formula\nauto_scale_formula = \"\"\"startingNumberOfVMs = 1;\n    maxNumberofVMs = 10;\n    pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second);\n    pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second));\n    \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples);\n    \\$NodeDeallocationOption = taskcompletion;\"\"\"\n\ncreate_pool_and_resource_file(startup_script; enable_auto_scale=true, auto_scale_formula=auto_scale_formula,            \n    auto_scale_evaluation_interval_minutes=5)",
            "title": "Pools with auto-scaling"
        },
        {
            "location": "/#resize-the-pool",
            "text": "Currently not supported.",
            "title": "Resize the pool"
        },
        {
            "location": "/#remote-function-calls-via-batch",
            "text": "",
            "title": "Remote function calls via batch"
        },
        {
            "location": "/#batchdef",
            "text": "Execute an expression under Main and on the batch workers of a (future) batch job that is executed from the same Julia session (equivalent to  @everywhere  for parallel Julia sessions).  @batchdef expr  @batchdef  can be used to define variables, functions or with  include  and  using  statements:  # Import packages\n@batchdef using LinearAlgebra, Random\n\n# Includes\n@batchdef include(\"testfile.jl\")\n\n# Define variables\n@batchdef A = ones(2, 2)\n\n# Define functions\n@batchdef hello_world(name) = print(\"Hello $name\")  You can define multiple expression with  @batchdef  using a  begin ... end  block:  @batchdef begin\n    A = ones(1, 1)\n    B = zeros(1, 1)\nend  Expressions that are tagged via  @batchdef  are collected by AzureClusterlessHPC and are used in subsequent batch job executions. To print the current collection of expressions, type  batch_show() . To reset the batch environment and remove all prior expressions from the call stack, use  batch_clear()  (or restart the Julia session).",
            "title": "\\@batchdef"
        },
        {
            "location": "/#batchexec",
            "text": "Execute an expression as a batch job (equivalent to  @spawn  for parallel Julia sessions).  @batchexec expr  The primary purpose of  @batchexec  is to execute functions that have been priorly defined with  @batchdef . E.g.  # Define function\n@batchdef function hello_world(name)\n    print(\"Hello $name\")\n    return \"Goodbye\"\nend\n\n# Call function via batch\nbctrl = @batchexec hello_world(\"Bob\")  Arguments for functions executed via  @batchexec  are always  passed by copy . This is important to keep in mind when passing large arguments to a function that is executed as a multi-task batch job, in which case arguments are copied to each task separately. To pass large arguments to a multi-task batch job, use the  @bcast  macro (see next section).  To execute a multi-task batch job, use the  pmap  function:  # Multi-task batch job\nbctrl = @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"])  The  @batchexec  macro returns a batch controller ( bctrl ) that can be used for the following actions:    Wait for all tasks of the batch job to finish:  wait_for_tasks_to_complete(bctrl)    Terminate the batch job:  terminate_job(bctrl)    Delete the batch job:  delete_job(bctrl)    Delete the pool:  delete_pool(bctrl)    Delete the blob container in which all temporary files are stored:  delete_container(bctrl)    Destroy all Azure resources associated with the batch controller (job, pool, container):  destroy!(bctrl)    Fetch the output of all tasks:  output = fetch(bctrl) . This operation is blocking and waits for all tasks to finish. The output is collected asynchonously in order of completion.    Fetch the output of task  i :  output = fetch(bctrl, i)  (blocking for that task).    Inplace fetch (all tasks). Returns output and overwrites the blob future in  bctrl.output :  output = fetch!(bctrl)  (blocking operation)    Inplace fetch (task  i ):  output = fetch!(bctrl, i)  (blocking for task  i )    Fetch output of all tasks and apply a reduction operation to the output (along tasks):  output_reduce = fetchreduce(bctrl; op=+)  (blocking)    Inplace fetch and reduce (overwrite  output_reduce ):  fetchreduce!(bctrl, output_reduce; op=+)  (blocking)    Limitations:    Function return arguments must be explicitley returned via the  return  statement. I.e., implicit returns in which the final function expression is automatically returned are not supported.    Functions executed via  @batchexec  can only have a single  return  argument. I.e. control structures such as  if ... else ... end  with multiple  return  statements are not supported and will throw an exception when fetching the output.    Function arguments are passed by copy, never by reference.",
            "title": "\\@batchexec"
        },
        {
            "location": "/#mpi-support",
            "text": "You can execute tasks via Julia MPI on either single VMs or on multiple VMs. To enable MPI on a single VM (shared memory parallelism), set the following variables in your  parameters.json  file:      \"_INTER_NODE_CONNECTION\": \"0\",\n    \"_MPI_RUN\": \"1\",\n    \"_NUM_NODES_PER_TASK\": \"1\",\n    \"_NUM_PROCS_PER_NODE\": \"2\",\n    \"_OMP_NUM_THREADS\": \"1\"  Note, that  \"_NUM_NODES_PER_TASK\"  must be set to  1  if  \"_INTER_NODE_CONNECTION\"  is set to  \"0\" .  \"_NUM_PROCS_PER_NODE\"  specifies the number of MPI ranks per node and  \"_OMP_NUM_THREADS\"  specifies the number of OpenMP threads per rank (if applicable).  To enable MPI tasks on multiple instances (distributed memory parallelism), set:      \"_INTER_NODE_CONNECTION\": \"1\",\n    \"_MPI_RUN\": \"1\",\n    \"_NUM_NODES_PER_TASK\": \"2\",\n    \"_NUM_PROCS_PER_NODE\": \"4\",\n    \"_OMP_NUM_THREADS\": \"1\"  The total number of MPI ranks for each task is given by  \"_NUM_NODES_PER_TASK\"  times  \"_NUM_PROCS_PER_NODE\" . E.g. in this example, each MPI task is executed on 2 nodes with 4 processes per node, i.e. 8 MPI ranks in total.  In your application, you need to load the Julia MPI package via  @batchdef . For a full MPI example, see  AzureClusterlessHPC/examples/mpi/julia_batch_mpi.ipynb .",
            "title": "MPI support"
        },
        {
            "location": "/#broadcasting",
            "text": "Broadcast an expression to all batch workers of (future) batch jobs and return a batch future. The batch future can be passed as a function argument instead of the variable.  batch_future = @bcast expr  The use of  @bcast  is recommended to pass large arguments to functions (e.g. arrays). This avoids copying input arguments to each individual task separately. Instead, expressions tagged via  @batchdef  are uploaded to blob storage once and their blob reference is passed to one or multiple tasks.  To access a broadcasted variable inside an executed function, use the  fetch  or  fetch!  (in-place) function:  # Create and broadcast array\nA = randn(2, 2)\n_A = @bcast A\n\n# Define function\n@batchdef function print_array(_A)\n    A = fetch(_A)   # load A into memory\n    print(A)\nend\n\n# Remotely execute function\n@batchexec print_array(_A)  # pass batch future  Calling  A = fetch(_A)  on the local machine (rather than on a batch worker) downloads the broadcasted variable from blob storage and returns it.",
            "title": "Broadcasting"
        },
        {
            "location": "/#collect-output",
            "text": "",
            "title": "Collect output"
        },
        {
            "location": "/#fetch-output",
            "text": "Executing a function as a batch job via  @batchexec  returns a batch controller of type  BatchController :  # Test function\n@batchdef function hello_world(n)\n    A = zeros(n, n)\n    B = ones(n, n)\n    return A, B\nend\n\n# Execute function as a multi-task batch job\nn = 2\nbatch_controller = @batchexec pmap(() -> hello_world(n), 1:2)  # 2 tasks  The batch controller has a field called  batch_controller.output , which is a cell array of blob futures. The blob futures contain a (randomly generated) blob name of the future result stored in blob storage. E.g.:  julia> batch_controller.output\n\n2-element Array{Any,1}:\n BlobFuture(\"redwoodtemp\", BlobRef((\"o9UspZStMmqn\", \"TwIMfLrYiac2\")))\n BlobFuture(\"redwoodtemp\", BlobRef((\"PxgtEgZonWPJ\", \"kZz1Wuknnag0\")))  The cell array contains one entry per task, i.e.  length(batch_controller.output)  is equal to the number of tasks of the executed batch job (in this case 2). As our function returns two arguments, each  BlobRef  contains two (future) blob names.  To fetch the output of an executed function, AzureClusterlessHPC provides the  fetch  and  fetch!  functions. These functions can be either called on the batch controller  output = fetch(batch_controller)  or they can be directly called on the blob futures:  # fetch called on batch controller\noutput_job = fetch(batch_controller)\n\n# fetch called on blob future\noutput_task_1 = fetch(batch_controller.output[1])  However, we recommend to always call  fetch  on the batch controller and not on the batch futures in  .output . Calling  fetch(batch_controller)  is a blocking operation and waits for all batch tasks to terminate. Calling  fetch(batch_controller.output[1])  is non-blocking and throws an exception if the task or job has not yet finished and the output is not yet available in blob storage.  AzureClusterlessHPC also supplies in-place fetch functions, which not only return the output, but they also overwrite the  BlobRef  of the  BlobFuture  in  batch_controller.output :  # Inplace fetch\noutput = fetch!(batch_controller)\n\n2-element Array{Any,1}:\n ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])\n ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])\n\nbatch_controller.output\n\n2-element Array{Any,1}:\n BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]))\n BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]))  Inplace  fetch!  by default deletes the referenced blob objects. If  fetch!  is called on the batch controller again, it will then throw an error. To avoid deleting the blob, call  fetch!(batch_controller; destroy_blob=false) .",
            "title": "Fetch output"
        },
        {
            "location": "/#fetch-output-and-apply-reduction-operation",
            "text": "AzureClusterlessHPC supplies the  fetchreduce  and  fetchreduce!  functions to collect the output from multiple tasks and apply a specified reduction operation to the output.\nE.g. using the prior example:  # Test function\n@batchdef function hello_world(n)\n    A = ones(n, n)\n    B = 2 .* ones(n, n)\n    return A, B\nend\n\n# Execute function as a multi-task batch job\nn = 2\nbatch_controller = @batchexec pmap(() -> hello_world(n), 1:2)  # 2 tasks  We can fetch and sum the output via:  output_sum = fetchreduce(batch_controller; op=+, remote=false)\n\n# Returns\n([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0])  The  remote  keyword argument specifies where the summation is execute. By default, the output is collected and summed on the master. For  remote=true , AzureClusterlessHPC will schedule the summation tasks on idle instances in the batch pool and only the final (reduced) argument is copied back to the master.  We can also initialize the output ourselves and then call the in-place  fetchreduce!  function:  # Initialize output\noutput = (zeros(2, 2), zeros(2, 2))\n\n# Fetch output and sum\nfetchreduce!(batch_controller, output; op=+)\n\n@show output\noutput = ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0])",
            "title": "Fetch output and apply reduction operation"
        },
        {
            "location": "/#clean-up-resources",
            "text": "After executing a batch job via  @batchexec , you can use the returned batch controller to clean up resources:  # Batch job\nbatch_controller = @batchexec print(\"Hello world\")\n\n# Terminate job\nterminate_job(batch_controller)\n\n# Delete job\ndelete_job(batch_controller)\n\n# Delete pool\ndelete_pool(batch_controller)\n\n# Delete blob container with all temporary files\ndelte_container(batch_controller)\n\n# Or alternatively, delete pool + job + container together\ndestroy!(batch_controller)  If you did not return a batch controller, you can call the following functions without any input arguments, in which case they will delete the pool and container as specified in your  parameter.json  file (or the default ones). The  delete_all_jobs  function will delete all exisiting jobs that start with the job id specified in the parameter file.  # Delete container\ndelete_container()\n\n# Delete pool\ndelete_pool()\n\n# Delete all jobs\ndelete_all_jobs()",
            "title": "Clean up resources"
        },
        {
            "location": "/#faq",
            "text": "How does AzureClusterlessHPC work?   Whenever you tag an expression with  @batchdef , AzureClusterlessHPC collects the abstract syntax tree (AST) of the expressions and appends it to a global collection. You can print the currently collected AST via  batch_show()  and you can reset the collected expressions via  batch_clear() . When you use  @batchexec , AzureClusterlessHPC creates a closure around the executed expression and uploads it, along with the collected AST as a batch resource file. AzureClusterlessHPC also anayzes the executed funtion and replaces return statements with serializations, so that return arguments are written to the local disk of the batch worker and subsequently uploaded to blob storage, from where they can be collected via the  fetch / fetch!  functions.   What costs does AzureClusterlessHPC incur?   AzureClusterlessHPC calls Azure Batch and Azure Blob Storage APIs. Costs incur for operations that write data to blob storage, download or store it (e.g.  @bcast ,  @batchexec ,  fetch ,  fetch! ). For batch jobs, costs incur for the requested VMs in the batch pool (regardless of whether jobs are currently running or not).    How do I clean up and shut down all services that invoke costs?   Costs are invoked by a batch pool made up of one or multiple VMs and by files stored in blob storage. To shut down the pool run  delete_pool  and to delete the blob container that contains any temporary files run  delete_container() . These actions will delete the pool and blob container specified in your parameter JSON file (or the default ones created by AzureClusterlessHPC).   How can I specify Julia packages to be installed on the batch worker nodes?   To specify Julia packages that are installed on the worker nodes, create a pool startup script and use the  create_pool_and_resource_file  function to launch the pool. Refer to the section \"Create a batch pool\" for details.   How can I start a pool with a custom VM image?   To start a pool with a custom VM image, you need to first create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see  here  for details on how to create a shared image). When starting your batch pool, pass this ID to the pool startup function:  create_pool(image_resource_id=\"shared_image_id\") .   What kind of input and return arguments are supported in functions executed via  @batchexec ?   AzureClusterlessHPC.jl supports any kind of input and return arguments, including custom data structures. Input and return arguments do not need to be JSON serializable. However, we recommend using the same Julia version on the batch workers as on your local machine or master VM. This avoids possible inconsistencies when serializing/deserializing arguments and expressions.   Are MPI and multi-node batch tasks supported?   Yes, you can execute AzureClusterlessHPC tasks via Julia MPI on either single VMs or on multiple VMs. See the above section  MPI support  for details on how to runs batch tasks with MPI support.",
            "title": "FAQ"
        },
        {
            "location": "/#troubleshooting",
            "text": "Contact the developer at  pwitte@microsoft.com .",
            "title": "Troubleshooting"
        },
        {
            "location": "/about/",
            "text": "About\n\n\nAzureClusterlessHPC.jl is developed and maintained by the \nMicrosoft Research for Industry\n (RFI) team.",
            "title": "About"
        },
        {
            "location": "/about/#about",
            "text": "AzureClusterlessHPC.jl is developed and maintained by the  Microsoft Research for Industry  (RFI) team.",
            "title": "About"
        }
    ]
}