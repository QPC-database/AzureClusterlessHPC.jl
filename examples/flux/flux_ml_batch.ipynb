{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Machine learning with Flux.jl and AzureClusterlessHPC.jl\n",
    "\n",
    "This tutorial demonstrates how to run a simple machine learning example with Julia's Flux library and AzureClusterlessHPC. We start by setting the environment variables for our credential and parameter files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for this example\n",
    "using Pkg; Pkg.add(\"Flux\")\n",
    "\n",
    "# Set paths to credentials + parameters\n",
    "ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"../..\", \"credentials.json\")\n",
    "ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\")\n",
    "\n",
    "# Load package\n",
    "using AzureClusterlessHPC\n",
    "batch_clear();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up a pool with 2 workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool 1 of 1 in canadacentral already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create pool\n",
    "startup_script = \"pool_startup_script_flux.sh\"\n",
    "create_pool_and_resource_file(startup_script);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the required packages and tag packages that we want to use on the batch workers with the `@batchdef` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "@batchdef using Flux\n",
    "@batchdef using Flux.Optimise: update!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function, which takes the current network, as well as the input `x` and output `y` as input arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "@batchdef function loss(model, x, y)\n",
    "    ŷ = model(x)\n",
    "    sum((y .- ŷ).^2)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the loss function only computes the function value, but not the gradients, we define an additional function called `objective`, which evaluates the loss function and then uses Flux' automatic differentiation to compute the gradients. We then collect all gradients in a cell array and return it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluation network and computing gradients\n",
    "@batchdef function objective(_model, x, y)\n",
    "\n",
    "    # Load model into memory\n",
    "    model = fetch(_model)\n",
    "    θ = params(model)\n",
    "\n",
    "    # Compute grad\n",
    "    gs = gradient(() -> loss(model, x, y), θ)\n",
    "\n",
    "    # Return cell array of local grads\n",
    "    grads = []\n",
    "    for p in θ\n",
    "        push!(grads, gs[p])\n",
    "    end\n",
    "    return grads\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our objective function in place, we now define our neural network. In this case, our network is a simple two-layer model with a softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "model = Chain(\n",
    "  Dense(10, 5, σ),\n",
    "  Dense(5, 2),\n",
    "  softmax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple training data set consisting of 100 training examples (i.e. matrix rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "ntrain = 100\n",
    "x = rand(10, ntrain)\n",
    "y = rand(2, ntrain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we use a simple SGD optimizer which we run for 2 iterations with a batch size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "opt = Descent(1f-3)\n",
    "maxiter = 2\n",
    "batchsize = 4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run our training loop. During each iteration, we select a random set of rows from the training data. Then, we broadcast the current version of our network to the batch workers, which returns a batch future `_model`. Next, we evaluate the objective function as a multi-task batch job, in which each task compute the gradient of the weights for one row of the current data batch. We then collect and sum all gradients into a single update and use it to update our (locally stored) network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "  6.396477 seconds (4.37 M allocations: 259.314 MiB, 2.32% gc time, 4.60% compilation time)\n",
      "Monitoring tasks for 'Completed' state, timeout in 60 minutes ...Uploading file model.dat to container [azureclusterlesstemp]...\n",
      "Creating job [FluxDeepLearning_SI77hbf2_1]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/application-cmd to blob container [azureclusterlesstemp]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/batch_runtime.jl to blob container [azureclusterlesstemp]...\n",
      "Uploading file packages.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_1.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_2.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_3.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_4.dat to container [azureclusterlesstemp]...\n",
      ".............................\n",
      "Fetch output from task task_4...............................\n",
      "Fetch output from task task_3...................................\n",
      "Fetch output from task task_2......................................\n",
      "Fetch output from task task_1\n",
      "Iteration 2\n",
      "  1.052258 seconds (2.13 k allocations: 154.172 KiB)\n",
      "Monitoring tasks for 'Completed' state, timeout in 60 minutes ...Uploading file model.dat to container [azureclusterlesstemp]...\n",
      "Creating job [FluxDeepLearning_rMZ8Gln8_1]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/application-cmd to blob container [azureclusterlesstemp]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/batch_runtime.jl to blob container [azureclusterlesstemp]...\n",
      "Uploading file packages.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_1.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_2.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_3.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_4.dat to container [azureclusterlesstemp]...\n",
      ".............................\n",
      "Fetch output from task task_4................................\n",
      "Fetch output from task task_3......................\n",
      "Fetch output from task task_2................\n",
      "Fetch output from task task_1\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for j=1:maxiter\n",
    "    print(\"Iteration $j\\n\")\n",
    "\n",
    "    # Current batch\n",
    "    idx = randperm(ntrain)[1:batchsize]\n",
    "    x_batch = x[:, idx]\n",
    "    y_batch = y[:, idx]\n",
    "\n",
    "    # Broadcast current version of network\n",
    "    _model = @bcast(model)\n",
    "\n",
    "    # Compute gradients using Azure Batch\n",
    "    bctrl = @batchexec pmap(i -> objective(_model, x_batch[:,i], y_batch[:,i]), 1:batchsize)\n",
    "    grad = fetchreduce(bctrl; op=+); delete_job(bctrl)\n",
    "\n",
    "    # Update local network parameters\n",
    "    for (p, g) in zip(params(model), grad)\n",
    "        update!(opt, p, g)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is the clean up of our resources. We delete the blob container that contains all temporary files and delete the batch pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete container and pool specified in the parameter json file\n",
    "delete_container()\n",
    "delete_all_jobs()\n",
    "delete_pool();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License (MIT). See LICENSE in the repo root for license information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
