{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Execute tasks via MPI on single or multiple batch nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AzureClusterlessHPC supports running single or multi-node tasks with distributed memory parallelism and Julia's MPI package. In combination with batch processing, AzureClusterlessHPC therefore allows users to execute multiple MPI tasks in parallel. Concurrent MPI tasks can be executed (a) in the same batch pool; (b) in separate batch pools beloning to the same batch account; (c) in separate batch pools belonging to separate batch accounts.\n",
    "\n",
    "## Set up\n",
    "\n",
    "To enable MPI jobs, set the following parameters in the `parameters.json` file:\n",
    "\n",
    "- `MPI_RUN`: Set to \"`1\"` to enable execution of tasks as MPI jobs (default is `\"0\"`). If set to `\"1\"`, AzureClusterlessHPC will execute the user code on the batch node via `mpiexecjl`.\n",
    "\n",
    "- `\"_INTER_NODE_CONNECTION\"`: Set to `\"1\"` if you want to run each MPI tasks across multiple nodes or set to `\"0\"` if you want to run each MPI task on a single node (e.g. for accessing multiple sockets on the same node).\n",
    "\n",
    "\n",
    "- `\"_NUM_NODES_PER_TASK\"`: The number of nodes per task. Note that if you set this parameter > 1, `\"_INTER_NODE_CONNECTION\"` must be set to `\"1\"` (otherwise AzureClusterlessHPC defaults to 1 node per task).\n",
    "\n",
    "\n",
    "- `\"_NUM_PROCS_PER_NODE\"`: Number of MPI ranks/processes per node. The total number of MPI ranks is given by `\"_NUM_NODES_PER_TASK\"` times `\"_NUM_PROCS_PER_NODE\"`. E.g., if you use two nodes per task and 4 processes per node, the total number of MPI ranks is 8.\n",
    "\n",
    "\n",
    "- `\"_OMP_NUM_THREADS\"`: Number of OpenMP threads (if applicable; default is `\"1\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Julia packages required for this example\n",
    "using Pkg; Pkg.add(\"MPI\")\n",
    "\n",
    "# Set path to credentials\n",
    "ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"../..\", \"credentials.json\")\n",
    "\n",
    "# Set path to batch parameters (pool id, VM types, etc.)\n",
    "ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\")\n",
    "\n",
    "# Load package\n",
    "using AzureClusterlessHPC\n",
    "batch_clear();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start our batch pool. If `\"_INTER_NODE_CONNECTION\"` is set to `\"1\"`, AzureClusterlessHPC enables inter-node communication in the batch pool(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created pool 1 of 2 in canadacentral with 2 nodes.\n",
      "Created pool 2 of 2 in canadacentral with 2 nodes.\n"
     ]
    }
   ],
   "source": [
    "# Create pool\n",
    "startup_script = \"pool_startup_script.sh\"\n",
    "create_pool_and_resource_file(startup_script);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI jobs with AzureClusterlessHPC\n",
    "\n",
    "To execute MPI function, we need to load the MPI package with `@batchdef` and call `MPI.Init()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@batchdef using MPI\n",
    "@batchdef MPI.Init();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our function that will be executed by Azure Batch through AzureClusterlessHPC. With MPI, this function can be executed on either a single or multiple batch nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel MPI function\n",
    "@batchdef function hello_world()\n",
    "    comm = MPI.COMM_WORLD\n",
    "    print(\"Hello world, I am rank $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\\n\")\n",
    "    MPI.Barrier(comm)\n",
    "    return \"Goodbye\"\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute a single instance of this function using the `@batchexec` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute one MPI job\n",
    "@batchexec hello_world();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also execute multiple version of the function in parallel using the `pmap` function. In this case, we execute two separate MPI jobs in parallel and collect their results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.617372 seconds (96.32 k allocations: 5.929 MiB, 6.65% compilation time)\n"
     ]
    }
   ],
   "source": [
    "# Execute multiple MPI jobs in parallel\n",
    "bctrl = @batchexec pmap(i -> hello_world(), 1:2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring tasks for 'Completed' state, timeout in 60 minutes ...Creating job [BatchJob_QWS4rqHk_1]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/application-cmd to blob container [azureclusterlesstemp]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/batch_runtime.jl to blob container [azureclusterlesstemp]...\n",
      "Uploading file packages.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_1.dat to container [azureclusterlesstemp]...\n",
      "Creating job [BatchJob_BYJEmazC_1]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/application-cmd to blob container [azureclusterlesstemp]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/batch_runtime.jl to blob container [azureclusterlesstemp]...\n",
      "Uploading file packages.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_1.dat to container [azureclusterlesstemp]...\n",
      "Creating job [BatchJob_BYJEmazC_2]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/application-cmd to blob container [azureclusterlesstemp]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/batch_runtime.jl to blob container [azureclusterlesstemp]...\n",
      "Uploading file packages.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_2.dat to container [azureclusterlesstemp]...\n",
      "...................................................................................................................................................\n",
      "Fetch output from task task_1...\n",
      "Fetch output from task task_2\n",
      "Any[\"Goodbye\", \"Goodbye\"]"
     ]
    }
   ],
   "source": [
    "# Collect output\n",
    "out = fetch(bctrl)\n",
    "print(out);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we clean up all consumed Azure resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "destroy!(bctrl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License (MIT). See LICENSE in the repo root for license information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
