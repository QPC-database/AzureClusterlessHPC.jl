{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A quick primer on distributed computing Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Julia language natively supports parallel and distributed computing via its `Distributed` package. Julia's distributed computing is based on one-sided communications and remote functions calls. We start by loading the `Distributed` package and by adding two Julia workers to the running session. (Here, all workers are running on our local PC, but if our Julia session was running on a cluster, these workers would be located on the worker nodes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "addprocs(2)\n",
    "@show nprocs();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages and libraries in Julia are importet via the `using` statement. To make loaded packages available on all workers and not just the master process, we have to tag our expression with the `@everywhere` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using LinearAlgebra, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that is callable both locally and on the remote workers works in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function everywhere\n",
    "@everywhere function hello_world(name1, name2; kwargs...)\n",
    "    print(\"Hello \", name1, \" and \", name2, \"\\n\")\n",
    "    return \"Goodbye from worker $(myid())\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function executes the above code on our (local) master process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function locally\n",
    "out = hello_world(\"Bob\", \"John\")\n",
    "@show out;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the function on a remote worker, we have to use another macro. By calling our function with the `@spawn` statement, we call the function on one of the workers from the current pool. The `@spawn` macro is non-blocking and immediately returns our call with a so-called `Future`, which points to a future result on the remote worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function on remote worker\n",
    "future = @spawn hello_world(\"Bob\", \"John\");\n",
    "@show typeof(future);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `fetch` function, we can copy the result from the remote process to the local memory. The `fetch` function is blocking and waits until the remote computation has been completed and the result is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fetch(future)\n",
    "@show out;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other macros and functions available in Julia to execute remote function calls. One of the important ones that we want to look at is the `pmap` function, which takes a custom function and applies it to all elements of an array using the current pool of workers. In the following example, we execute our `hello_world` function twice by looping over the list of input arguments. Unlike `@spawn`, `pmap` is blocking and returns a list of results after all workers have terminted their computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel map\n",
    "out = pmap(name -> hello_world(name, \"Jane\"), [\"Bob\", \"John\"])\n",
    "@show out;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote functions call to Azure Batch with AzureClusterlessHPC.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will use a similar set of macros provided by our package `AzureClusterlessHPC.jl` to execute remote function calls via Azure Batch. First, we have to provide user credentials for our Azure Batch and Storage accounts. Furthermore, we read a set of parameters specifying our batch setup; namely the pool name, job name, VM type and number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to credentials\n",
    "ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"credentials.json\")\n",
    "\n",
    "# Set path to batch parameters (pool id, VM types, etc.)\n",
    "ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\")\n",
    "\n",
    "# Load package\n",
    "using AzureClusterlessHPC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start our batch pool and pass it a startup shell script, which optionally let's us include Julia packages that are installed on the nodes in the pool. Here, we don't need any additionally packages and use a default startup script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startup_script = \"pool_startup_script.sh\"\n",
    "create_pool_and_resource_file(startup_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make code, packages and variables available on remote workers in the batch pool, `AzureClusterlessHPC` provides the `@batchdef` macro. Just as the `@everywhere` macro, it allows us to tag package imports and function definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@batchdef using LinearAlgebra, Random, Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@batchdef function hello_batch(myid, name1, name2; kwargs...)\n",
    "    print(\"Hello $name1 and $name2 from worker $myid.\\n\")\n",
    "    return \"Goodbye from worker $myid\" \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, calling the above function without any macros executes the function on our local PC or VM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hello_batch(1, \"Bob\", \"John\")\n",
    "@show out;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute functions remotely on a batch worker, `AzureClusterlessHPC` provides a similar macro to `@spawn` called `@batchexec`. By executing our function with this macro, our function call is submitted and executed as an Azure Batch job. Like `@spawn`, the macro is non-blocking and returns a batch control panel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bctrl = @batchexec hello_batch(1, \"Bob\", \"John\")\n",
    "@show typeof(bctrl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch controller contains some basic information about the batch job such as the pool and job ids and provides some basic functionalities like `terminate_job` or `delete_job`. Furthermore, it has a field `bctrl.output` which contains a Julia Future to the function output. As before, we can copy the result to the local memory using the (blocking) `fetch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fetch(bctrl)\n",
    "@show out;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete the job by applying the `delete_job` function to the batch controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_job(bctrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AzureClusterlessHPC.jl` also provides functionalities to run `pmap` commands with Azure Batch. However, unlike Julia's basic `pmap` function, which automatically executes the function on remote workers, `pmap` has to be tagged with `@batchexec` in order to execute the function call with Azure Batch. (Calling `pmap` without `@batchexec` will execute the function in the local worker pool instead.)\n",
    "\n",
    "Whereas the basic `pmap` function is a blocking call that returns a list of the worker outputs, calling `pmap` with `@batchexec` is non-blocking and returns a batch controller. Here we execute our `hello_batch` function four times as a multi-task Azure Batch job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bctrl = @batchexec pmap(idx -> hello_batch(idx, \"Bob\", \"John\"), 1:4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can copy the output to the local memory via the `fetch` function. We can either fetch the output of a specific task by calling `fetch(bctrl, 1)` (to fetch the result from task 1), or we call `fetch` without a task id, in which case we wait for all tasks to terminate and fetch their results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fetch(bctrl); delete_job(bctrl)\n",
    "@show out;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return arguments are not limited to simple strings or variables, but they can also included arrays or custom data structures. For example, we can define a custom structure `MyStruct` on the local machine as well as on the batch workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@batchdef struct MyStruct\n",
    "    a\n",
    "    b\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a new function, which returns a double precision array, as well as an instance of our custom class, containing an integer and a single precision array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "@batchdef function hello_earth(name1, name2; kwargs...)\n",
    "\n",
    "    print(\"Hello \", name1, \" and \", name2, \"\\n\")\n",
    "    print(\"kwargs: \", kwargs..., \"\\n\")\n",
    "    \n",
    "    # Create some random output\n",
    "    out1 = randn(2,2)\n",
    "    out2 = MyStruct(4, ones(Float32, 2,3))\n",
    "\n",
    "    return out1, out2\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@batchexec` macro supports generic Julia function calls, including optional and keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task batch job via pmap\n",
    "kwargs = (kw1 = \"one\", kw2 = \"two\")\n",
    "bctrl = @batchexec pmap(name -> hello_earth(name, \"Bob\"; kwargs...), [\"Jane\", \"John\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous example, we can copy the function output to the local memory using `fetch`. The only restirction is, that the structure/class of the return argument is also known on our local worker (which it is in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fetch(bctrl)\n",
    "@show out;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean up our Azure resources, we apply the `destroy!` function to our batch controller, which deletes the jobs, removes the blob container with temporary files and shuts down the pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destroy!(bctrl);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}